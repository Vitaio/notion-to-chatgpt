 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/app.py b/app.py
index 47bfb2c0d6c85c8da05b9151734c749565b65a87..f4ff4407b7a86baf1b284b565f83c4718cdeb1d5 100644
--- a/app.py
+++ b/app.py
@@ -1,50 +1,55 @@
 import io
 import os
 import re
 import csv
 import json
 import time
+import html
 import zipfile
 import unicodedata
 from datetime import datetime
 from typing import List, Dict, Tuple, Optional
 
 import streamlit as st
 
 # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 # Streamlit config
 # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 
 st.set_page_config(
     page_title="Notion â†’ Markdown/JSONL/CSV konverter",
     page_icon="ðŸ“¦",
     layout="centered",
 )
 
 st.title("ðŸ“¦ Notion â†’ Markdown/JSONL/CSV konverter")
-st.caption("Notion Markdown exportbÃ³l kinyeri a **VideÃ³/Lecke** szÃ¶veget (PONTOS H2 egyezÃ©ssel), tisztÃ­t, chunkol (opcionÃ¡lis), Ã©s tÃ¡blÃ¡zat-kivonatot kÃ©szÃ­t.")
+st.caption(
+    "Notion Markdown exportbÃ³l kinyeri az Ã¶sszes **VideÃ³ szÃ¶veg** lenyÃ­lÃ³ blokk tartalmÃ¡t,"
+    " lÃ¡tvÃ¡nyosabb, Ã¡tlÃ¡thatÃ³bb MD-t kÃ©szÃ­t (cÃ­msorok/listÃ¡k rendezÃ©se), opcionÃ¡lisan chunkol,"
+    " Ã©s tÃ¡blÃ¡zat-kivonatot kÃ©szÃ­t."
+)
 
 # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 # Kis segÃ©dek
 # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 
 def run_id() -> str:
     return datetime.now().strftime("%Y%m%d_%H%M%S")
 
 
 def normalize(s: str) -> str:
     if not s:
         return ""
     s = s.strip().lower()
     s = "".join(ch for ch in unicodedata.normalize("NFKD", s) if not unicodedata.combining(ch))
     s = re.sub(r"[^a-z0-9]+", " ", s).strip()
     return s
 
 
 def slugify(s: str) -> str:
     s = normalize(s)
     s = re.sub(r"\s+", "-", s)
     s = re.sub(r"[^a-z0-9\-]+", "", s)
     return s or "doc"
 
 
@@ -122,78 +127,145 @@ def split_markdown_sections(md: str) -> List[Tuple[int, str, List[str]]]:
 
     for ln in lines:
         m = HEADING_RE.match(ln)
         if m:
             # Ãºj szekciÃ³
             flush()
             current_level = len(m.group(1))
             current_title = m.group(2).strip()
             current_buf = []
         else:
             if current_level == 0:
                 # heading elÅ‘tt/utÃ¡n Ã¡llÃ³ tartalom (H1 elÅ‘tti rÃ©sz)
                 current_title = ""
                 current_level = 0
                 current_buf = []
             current_buf.append(ln)
 
     flush()
     return sections
 
 # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 # PONTOS H2-egyezÃ©shez szÃ¼ksÃ©ges konstansok/fÃ¼ggvÃ©nyek
 # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 
 EXACT_VIDEO_HEADING = "VideÃ³ szÃ¶veg"
-EXACT_LESSON_HEADING = "Lecke szÃ¶veg"
-_H2_ANY = re.compile(r"^##\s+.+$", flags=re.MULTILINE)
+_DETAILS_RE = re.compile(
+    r"<details\b[^>]*>\s*(.*?)</details\s*>", flags=re.DOTALL | re.IGNORECASE
+)
+_SUMMARY_RE = re.compile(
+    r"<summary\b[^>]*>\s*(.*?)</summary\s*>", flags=re.DOTALL | re.IGNORECASE
+)
+
 
-def _extract_section_exact_h2(md: str, heading: str) -> str:
+def _html_to_markdownish(fragment: str) -> str:
     """
-    Csak a PONTOSAN '## <heading>' cÃ­msor alatti tartalmat adja vissza a kÃ¶vetkezÅ‘ H2-ig.
-    Ha nincs ilyen cÃ­msor vagy nincs Ã©rdemi tartalom, Ã¼res stringet ad vissza.
+    EgyszerÅ± HTMLâ†’Markdown-szerÅ± Ã¡talakÃ­tÃ¡s a toggle-blokkokhoz, hogy a sortÃ¶rÃ©sek,
+    cÃ­msorok Ã©s listÃ¡k olvashatÃ³bbak legyenek.
     """
-    md = md or ""
-    m = re.search(rf"^##\s*{re.escape(heading)}\s*$", md, flags=re.MULTILINE)
-    if not m:
+    if not fragment:
+        return ""
+
+    txt = fragment
+    replacements = [
+        (r"<br\s*/?>", "\n"),
+        (r"</p\s*>", "\n\n"),
+        (r"<p[^>]*>", ""),
+        (r"</li\s*>", "\n"),
+        (r"<li[^>]*>", "- "),
+        (r"</(ul|ol)\s*>", "\n"),
+        (r"<(ul|ol)[^>]*>", ""),
+        (r"<h1[^>]*>(.*?)</h1\s*>", r"# \1\n\n"),
+        (r"<h2[^>]*>(.*?)</h2\s*>", r"## \1\n\n"),
+        (r"<h3[^>]*>(.*?)</h3\s*>", r"### \1\n\n"),
+        (r"<h4[^>]*>(.*?)</h4\s*>", r"#### \1\n\n"),
+        (r"<h5[^>]*>(.*?)</h5\s*>", r"##### \1\n\n"),
+        (r"<h6[^>]*>(.*?)</h6\s*>", r"###### \1\n\n"),
+    ]
+    for pat, repl in replacements:
+        txt = re.sub(pat, repl, txt, flags=re.IGNORECASE)
+
+    # minden mÃ¡s HTML tag eltÃ¡volÃ­tÃ¡sa, entitÃ¡sok feloldÃ¡sa
+    txt = re.sub(r"<[^>]+>", "", txt)
+    txt = html.unescape(txt)
+
+    lines = [ln.rstrip() for ln in txt.splitlines()]
+    while lines and not lines[0].strip():
+        lines.pop(0)
+    while lines and not lines[-1].strip():
+        lines.pop()
+    return "\n".join(lines).strip()
+
+def _extract_video_toggle(md: str) -> str:
+    """
+    KizÃ¡rÃ³lag a 'VideÃ³ szÃ¶veg' feliratÃº lenyÃ­lÃ³ (toggle) blokk(ok) tartalmÃ¡t adja vissza.
+    - tolerÃ¡lja a <details> Ã©s </summary> kÃ¶rÃ¼li whitespace-et
+    - a summary HTML-je normalizÃ¡lva hasonlÃ­t, Ã­gy a dÃ­szÃ­tÅ‘ tagek sem zavarjÃ¡k
+    - blockquote / behÃºzott toggle is mÅ±kÃ¶dik (a sor eleji '>' Ã©s whitespace lecsupaszÃ­tÃ¡sÃ¡val)
+    - a tartalom HTML-bÅ‘l Markdown-szerÅ±re konvertÃ¡lva kerÃ¼l vissza,
+      hogy a cÃ­msorok, felsorolÃ¡sok, sortÃ¶rÃ©sek megmaradjanak
+    """
+    if not md:
+        return ""
+
+    # Ha a toggle blockquote-ban/behÃºzva Ã¡ll, pucoljuk le a sor elejÃ©rÅ‘l a dÃ­szÃ­tÃ©st
+    normalized_md = "\n".join(line.lstrip(" >\t") for line in md.splitlines())
+
+    parts = []
+
+    for details_match in _DETAILS_RE.finditer(normalized_md):
+        block = details_match.group(1)
+        summary_match = _SUMMARY_RE.search(block)
+        if not summary_match:
+            continue
+
+        summary_text = _html_to_markdownish(summary_match.group(1))
+        if normalize(summary_text) != normalize(EXACT_VIDEO_HEADING):
+            continue
+
+        content_html = block[summary_match.end():]
+        content_md = _html_to_markdownish(content_html)
+
+        # ha a konverziÃ³ Ã¼res lenne (pl. csak tagek), essÃ¼nk vissza a nyers, tag-mentesÃ­tett tartalomra
+        if not content_md:
+            content_md = html.unescape(re.sub(r"<[^>]+>", "", content_html)).strip()
+
+        if content_md:
+            parts.append(content_md)
+
+    if not parts:
         return ""
-    start = m.end()
-    m2 = _H2_ANY.search(md, pos=start)
-    end = m2.start() if m2 else len(md)
-    return md[start:end].strip()
+    return "\n\n".join(parts)
 
 def choose_section_exact(md: str) -> Tuple[str, str, str]:
     """
-    PrioritÃ¡s: VideÃ³ szÃ¶veg > Lecke szÃ¶veg; egyik sincs â†’ none.
+    Csak a 'VideÃ³ szÃ¶veg' lenyÃ­lÃ³ blokk tartalmÃ¡t vÃ¡lasztja ki.
     Vissza: (selected_section, raw_text, selected_heading)
     """
-    video = _extract_section_exact_h2(md, EXACT_VIDEO_HEADING)
-    lesson = _extract_section_exact_h2(md, EXACT_LESSON_HEADING)
+    video = _extract_video_toggle(md)
     if video:
         return "video", video, EXACT_VIDEO_HEADING
-    if lesson:
-        return "lecke", lesson, EXACT_LESSON_HEADING
     return "none", "", ""
 
 # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 # Markdown tisztÃ­tÃ¡s
 # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 
 def clean_markdown(md: str) -> str:
     if not md:
         return ""
     # headingek elÅ‘tt 1 Ã¼res sor, kÃ³dblokkok megkÃ­mÃ©lÃ©se, Ã¼res sorok normalizÃ¡lÃ¡sa
     out = []
     in_code = False
     fence = re.compile(r"^\s*```")
     prev_blank = True
     for line in md.splitlines():
         if fence.match(line):
             in_code = not in_code
             out.append(line)
             continue
         if in_code:
             out.append(line)
             continue
         if HEADING_RE.match(line):
             if not prev_blank:
                 out.append("")
@@ -233,50 +305,105 @@ def renumber_ordered_lists(md: str) -> str:
             continue
         if in_code:
             out.append(line)
             continue
 
         m = list_item.match(line)
         if m:
             indent = m.group(1)
             lvl = level_of(indent)
             if lvl not in counters:
                 counters[lvl] = 1
             else:
                 counters[lvl] += 1
             # nullÃ¡zÃ¡s mÃ©lyebb szinteken
             for k in list(counters.keys()):
                 if k > lvl:
                     del counters[k]
             newnum = counters[lvl]
             # FIX: \1 helyett \g<1>, hogy ne legyen \11, \110 stb. csoport hivatkozÃ¡s
             line = list_item.sub(r"\g<1>{0}. ".format(newnum), line, count=1)
             out.append(line)
         else:
             out.append(line)
     return "\n".Join(out).strip() if False else "\n".join(out).strip()  # vÃ©dÅ‘hack: ne tÃ¶rÃ¶ld a sort
 
+
+def enhance_readability(md: str) -> str:
+    """
+    EgyszerÅ±sÃ­tett formÃ¡zÃ¡s a jobb Ã¡ttekinthetÅ‘sÃ©ghez:
+    - egysÃ©ges "- " jelÃ¶lÃ©s a felsorolÃ¡soknÃ¡l,
+    - Ã¼res sor beillesztÃ©se listÃ¡k Ã©s cÃ­msorok elÃ©,
+    - a cÃ­msorok utÃ¡n egy Ã¼res sort hagy, hogy elkÃ¼lÃ¶nÃ¼ljenek.
+    """
+    if not md:
+        return ""
+
+    lines = md.splitlines()
+    out: List[str] = []
+
+    ul_re = re.compile(r"^(\s*)[-*+]\s+(.*)$")
+    ol_re = re.compile(r"^(\s*)\d+\.\s+(.*)$")
+
+    for i, line in enumerate(lines):
+        heading = HEADING_RE.match(line)
+        ul = ul_re.match(line)
+        ol = ol_re.match(line)
+
+        if heading:
+            if out and out[-1] != "":
+                out.append("")
+            out.append(line.rstrip())
+            out.append("")
+            continue
+
+        if ul:
+            indent, rest = ul.groups()
+            if out and out[-1] != "":
+                out.append("")
+            out.append(f"{indent}- {rest.strip()}")
+            continue
+
+        if ol:
+            indent, rest = ol.groups()
+            if out and out[-1] != "":
+                out.append("")
+            out.append(f"{indent}1. {rest.strip()}")
+            continue
+
+        if line.strip() == "":
+            if out and out[-1] == "":
+                continue
+            out.append("")
+        else:
+            out.append(line.rstrip())
+
+    while out and out[-1] == "":
+        out.pop()
+
+    return "\n".join(out)
+
 def strip_bold_emphasis(md: str) -> str:
     """
     EltÃ¡volÃ­tja a **â€¦** Ã©s __â€¦__ kiemelÃ©st kÃ³dblokkokon kÃ­vÃ¼l,
     a tartalmat meghagyva (gÃ©pi feldolgozÃ¡st segÃ­ti).
     """
     if not md:
         return ""
     lines = md.splitlines()
     out = []
     in_code = False
     fence = re.compile(r"^\s*```")
     bold_ast = re.compile(r"(?<!\*)\*\*(.+?)\*\*(?!\*)")
     bold_uscr = re.compile(r"(?<!_)__(.+?)__(?!_)")
     for line in lines:
         if fence.match(line):
             in_code = not in_code
             out.append(line)
             continue
         if in_code:
             out.append(line)
             continue
         # inline code vÃ©delme: daraboljuk backtick alapjÃ¡n
         parts = re.split(r"(`[^`]*`)", line)
         for i, part in enumerate(parts):
             if i % 2 == 0:  # nem inline code
@@ -580,60 +707,60 @@ def convert_zip_to_datasets(
         "meta_tipus", "meta_kurzus", "meta_vimeo_link", "meta_sorszam"
     ])
     rep_w.writerow(["file_name", "page_id", "page_title", "video_len", "lesson_len", "selected", "selected_len"])
 
     # TisztÃ­tott MD-k kÃ¼lÃ¶n ZIP-be
     md_zip_buf = io.BytesIO()
     md_zip = zipfile.ZipFile(md_zip_buf, "w", compression=zipfile.ZIP_DEFLATED)
     used_names = set()  # â† Ã¼tkÃ¶zÃ©skezelÃ©s a ZIP-ben
 
     # TÃ¡blÃ¡zatok kÃ¼lÃ¶n JSONL-be (Ã¶sszes dokumentum)
     tables_jsonl_buf = io.StringIO()
 
     total = len(md_files)
     ok = 0
     skipped = 0  # nincs szÅ±rÃ©s, nem nÅ‘
     progress = st.progress(0.0, text=f"0/{total} feldolgozva (âœ…: 0, kihagyva: 0)")
 
     for idx, (fname, text) in enumerate(md_files, start=1):
         page_id = extract_page_id_from_filename(fname) or ""
         title = extract_page_title(text, fallback=os.path.splitext(os.path.basename(fname))[0])
 
         # Metaadatok a H1 utÃ¡ni blokkbÃ³l
         meta = parse_metadata_block(text)
         sorsz_int = meta_sorszam_as_int(meta)
 
-        # PONTOS H2 egyezÃ©s (csak a kÃ©t fix cÃ­m engedÃ©lyezett)
-        video_txt  = _extract_section_exact_h2(text, EXACT_VIDEO_HEADING)
-        lesson_txt = _extract_section_exact_h2(text, EXACT_LESSON_HEADING)
+        # LenyÃ­lÃ³ (toggle) VideÃ³ szÃ¶veg blokk kinyerÃ©se
+        video_txt = _extract_video_toggle(text)
 
-        # KivÃ¡lasztÃ¡s prioritÃ¡ssal
+        # KivÃ¡lasztÃ¡s: csak a lenyÃ­lÃ³ VideÃ³ szÃ¶veg tartalma szÃ¡mÃ­t
         selected, raw, selected_heading = choose_section_exact(text)
 
         # tisztÃ­tÃ¡s
         raw_clean = strip_bold_emphasis(raw)
         raw_clean = clean_markdown(raw_clean)
+        raw_clean = enhance_readability(raw_clean)
         raw_clean = renumber_ordered_lists(raw_clean)
 
         # tÃ¡blÃ¡zatok kivonata csak a kivÃ¡lasztott szÃ¶vegbÅ‘l
         md_with_tables, tables = extract_tables(raw_clean if raw_clean else "")
         if tables:
             # tÃ¡blÃ¡k JSONL â€“ globÃ¡lis gyÅ±jtÅ‘
             for t in tables:
                 tables_jsonl_buf.write(json.dumps({
                     "run_id": rid,
                     "page_id": page_id,
                     "file_name": os.path.basename(fname),
                     "page_title": title,
                     "selected_section": selected,
                     "selected_heading": selected_heading,
                     "table": t
                 }, ensure_ascii=False) + "\n")
 
         # JSONL rekord(ok)
         base_rec = {
             "run_id": rid,
             "doc_id": slugify(title) if not page_id else f"{slugify(title)}_{page_id[:8]}",
             "page_id": page_id,
             "file_name": os.path.basename(fname),
             "page_title": title,
             "selected_section": selected,
@@ -663,51 +790,51 @@ def convert_zip_to_datasets(
         else:
             rec = dict(base_rec)
             rec.update({"text_markdown": md_with_tables})
             jsonl_buf.write(json.dumps(rec, ensure_ascii=False) + "\n")
 
         # CSV sor
         csv_w.writerow([
             os.path.basename(fname),
             page_id,
             title,
             selected,
             selected_heading,
             len(md_with_tables),
             md_with_tables,
             base_rec["meta_szakasz"], base_rec["meta_video_statusz"], base_rec["meta_lecke_hossza"],
             base_rec["meta_utolso_modositas"], base_rec["meta_tipus"], base_rec["meta_kurzus"],
             base_rec["meta_vimeo_link"], base_rec["meta_sorszam"]
         ])
 
         # Riport
         rep_w.writerow([
             os.path.basename(fname),
             page_id,
             title,
             len(video_txt),
-            len(lesson_txt),
+            0,
             selected,
             len(md_with_tables)
         ])
 
         # â”€â”€ TisztÃ­tott MD kÃ©szÃ­tÃ©se meta blokkal a H1 utÃ¡n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         md_name_base = build_md_filename(title, sorsz_int, page_id, meta.get("kurzus") or "")
         md_name = uniquify_filename(md_name_base, used_names, page_id)
 
         # Meta cÃ­mkÃ©k megjelenÃ­tÃ©si sorrendben
         meta_labels = [
             ("Szakasz", "szakasz"),
             ("VideÃ³ stÃ¡tusz", "video_statusz"),
             ("Lecke hossza", "lecke_hossza"),
             ("UtolsÃ³ mÃ³dosÃ­tÃ¡s", "utolso_modositas"),
             ("TÃ­pus", "tipus"),
             ("Kurzus", "kurzus"),
             ("Vimeo link", "vimeo_link"),
         ]
         meta_lines = []
         for label, key in meta_labels:
             val = (meta.get(key) or "").strip()
             if val:
                 meta_lines.append(f"{label}: {val}")
 
         md_lines = []
@@ -726,53 +853,53 @@ def convert_zip_to_datasets(
         md_zip.writestr(md_name, clean_md_text.encode("utf-8"))
 
         ok += 1
         pct = idx / max(1, total)
         progress.progress(pct, text=f"{idx}/{total} feldolgozva (âœ…: {ok}, kihagyva: {skipped})")
 
     # ZÃ¡rÃ¡sok Ã©s kimenetek elÅ‘Ã¡llÃ­tÃ¡sa
     md_zip.close()
     clean_md_zip_bytes = md_zip_buf.getvalue()
 
     jsonl_bytes = (jsonl_buf.getvalue()).encode("utf-8")
     csv_bytes_bom = ("\ufeff" + csv_buf.getvalue()).encode("utf-8")     # BOM
     rep_bytes_bom = ("\ufeff" + rep_buf.getvalue()).encode("utf-8")     # BOM
     tables_jsonl_bytes = (tables_jsonl_buf.getvalue()).encode("utf-8")
 
     return jsonl_bytes, csv_bytes_bom, rep_bytes_bom, clean_md_zip_bytes, tables_jsonl_bytes
 
 
 # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 # UI
 # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 
 with st.expander("Mi ez?"):
     st.markdown(
         "- TÃ¶lts fel egy **Notion export ZIP**-et (Markdown & CSV exportbÃ³l a ZIP-et hasznÃ¡ld).\n"
-        "- A konverter **PONTOS egyezÃ©ssel** csak a `## VideÃ³ szÃ¶veg` vagy, ha az Ã¼res/hiÃ¡nyzik, a `## Lecke szÃ¶veg` szakaszt veszi ki.\n"
-        "- Ha egyik sincs, a kimenet: _Ehhez a leckÃ©hez nem kÃ©szÃ¼lt leÃ­rÃ¡s._\n"
-        "- A fÃ©lkÃ¶vÃ©r (**â€¦**) jelÃ¶lÃ©st eltÃ¡volÃ­tja (kÃ³dblokkok Ã©rintetlenek).\n"
+        "- A konverter az Ã¶sszes `VideÃ³ szÃ¶veg` lenyÃ­lÃ³ (toggle) blokk teljes tartalmÃ¡t veszi ki.\n"
+        "- Ha nincs ilyen lenyÃ­lÃ³ blokk, a kimenet: _Ehhez a leckÃ©hez nem kÃ©szÃ¼lt leÃ­rÃ¡s._\n"
+        "- A fÃ©lkÃ¶vÃ©r (**â€¦**) jelÃ¶lÃ©st eltÃ¡volÃ­tja (kÃ³dblokkok Ã©rintetlenek), a cÃ­msorokat Ã©s listÃ¡kat jobban tagolja az olvashatÃ³sÃ¡gÃ©rt.\n"
         "- A tÃ¡blÃ¡zatokat (GFM) felismeri Ã©s **JSON kivonatot** kÃ©szÃ­t rÃ³luk.\n"
         "- **Metaadatok megÅ‘rzÃ©se**: a *Szakasz, VideÃ³ stÃ¡tusz, Lecke hossza, UtolsÃ³ mÃ³dosÃ­tÃ¡s, TÃ­pus, Kurzus, Vimeo link* sorok a H1 utÃ¡n bekerÃ¼lnek a tisztÃ­tott MD-be.\n"
         "- A tisztÃ­tott MD fÃ¡jlnÃ©v sÃ©mÃ¡ja: `Kurzus - SorszÃ¡m - NÃ©v.md`.\n"
         "- Kimenet: **tisztÃ­tott MD-k (ajÃ¡nlott)** + haladÃ³ formÃ¡tumok: JSONL, CSV, riport CSV, tÃ¡blÃ¡zatok JSONL.\n"
         "- OpcionÃ¡lis: **chunkolÃ¡s** Ã¡tfedÃ©ssel (JSONL-hoz)."
     )
 
 st.sidebar.header("BeÃ¡llÃ­tÃ¡sok")
 do_chunk = st.sidebar.checkbox("JSONL chunkolÃ¡sa", value=True)
 target_chars = st.sidebar.number_input("Chunk cÃ©lszÃ©lessÃ©g (karakter)", min_value=1000, max_value=20000, value=5500, step=500)
 overlap_chars = st.sidebar.number_input("Chunk Ã¡tfedÃ©s (karakter)", min_value=0, max_value=5000, value=400, step=50)
 
 uploaded = st.file_uploader("TÃ¶ltsd fel a Notion Markdown ZIP-et", type=["zip"])
 
 if uploaded is not None:
     try:
         b = uploaded.read()
         jsonl_bytes, csv_bytes_bom, rep_bytes_bom, md_zip_bytes, tables_jsonl_bytes = convert_zip_to_datasets(
             b, do_chunk, target_chars, overlap_chars
         )
     except zipfile.BadZipFile:
         st.error("HibÃ¡s ZIP fÃ¡jl.")
         st.stop()
     except Exception as e:
         st.error(f"VÃ¡ratlan hiba: {e}")
 
EOF
)
