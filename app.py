import io
import os
import re
import csv
import json
import time
import zipfile
import unicodedata
from datetime import datetime
from typing import List, Dict, Tuple, Optional

import streamlit as st

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Streamlit config
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(
    page_title="Notion â†’ Markdown/JSONL/CSV konverter",
    page_icon="ğŸ“¦",
    layout="centered",
)

st.title("ğŸ“¦ Notion â†’ Markdown/JSONL/CSV konverter")
st.caption(
    "Notion Markdown exportbÃ³l kinyeri az Ã¶sszes **VideÃ³ szÃ¶veg** lenyÃ­lÃ³ blokk tartalmÃ¡t,"
    " lÃ¡tvÃ¡nyosabb, Ã¡tlÃ¡thatÃ³bb MD-t kÃ©szÃ­t (cÃ­msorok/listÃ¡k rendezÃ©se), opcionÃ¡lisan chunkol,"
    " Ã©s tÃ¡blÃ¡zat-kivonatot kÃ©szÃ­t."
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Kis segÃ©dek
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_id() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def normalize(s: str) -> str:
    if not s:
        return ""
    s = s.strip().lower()
    s = "".join(ch for ch in unicodedata.normalize("NFKD", s) if not unicodedata.combining(ch))
    s = re.sub(r"[^a-z0-9]+", " ", s).strip()
    return s


def slugify(s: str) -> str:
    s = normalize(s)
    s = re.sub(r"\s+", "-", s)
    s = re.sub(r"[^a-z0-9\-]+", "", s)
    return s or "doc"


def safe_filename_preserve_accents(s: str) -> str:
    # fÃ¡jlnÃ©vhez engedjÃ¼k az Ã©kezeteket, de szÅ±rjÃ¼k az egyÃ©b nem kÃ­vÃ¡nt karaktereket
    s = s.strip().replace("/", "-").replace("\\", "-")
    s = re.sub(r"[^\w\-\.\sÃÃ‰ÃÃ“Ã–ÅÃšÃœÅ°Ã¡Ã©Ã­Ã³Ã¶Å‘ÃºÃ¼Å±]+", "", s, flags=re.UNICODE)
    s = re.sub(r"\s+", " ", s).strip()
    return s or "file"


def build_md_filename(title: str, sorsz_int: Optional[int], page_id: Optional[str], kurzus: Optional[str] = None) -> str:
    """
    KÃ©rt sÃ©ma: 'Kurzus - SorszÃ¡m - NÃ©v.md'
    - ha bÃ¡rmelyik hiÃ¡nyzik, kulturÃ¡ltan kihagyjuk
    """
    parts = []
    if kurzus:
        parts.append(safe_filename_preserve_accents(kurzus))
    if sorsz_int is not None:
        parts.append(str(sorsz_int))
    if title:
        parts.append(safe_filename_preserve_accents(title))
    base = " - ".join(parts) if parts else "cikk"
    return f"{base}.md"


def uniquify_filename(name: str, used: set, page_id: Optional[str] = None) -> str:
    """
    Ha mÃ¡r lÃ©tezik adott nÃ©v a ZIP-ben, egÃ©szÃ­tsÃ¼k ki rÃ¶vid page_id-vel,
    ha az is Ã¼tkÃ¶zik, tegyÃ¼nk sorszÃ¡mozott zÃ¡rÃ³jelet.
    """
    if name not in used:
        used.add(name)
        return name
    stem, ext = os.path.splitext(name)
    if page_id:
        candidate = f"{stem} - {page_id[:8]}{ext}"
        if candidate not in used:
            used.add(candidate)
            return candidate
    i = 2
    while True:
        candidate = f"{stem} ({i}){ext}"
        if candidate not in used:
            used.add(candidate)
            return candidate
        i += 1


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Markdown szekcionÃ¡lÃ¡s
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HEADING_RE = re.compile(r"^(#{1,6})\s+(.*)$")

def split_markdown_sections(md: str) -> List[Tuple[int, str, List[str]]]:
    """
    Vissza: [(szint, heading, sorok)], ahol 'sorok' a heading utÃ¡ni tartalom a kÃ¶vetkezÅ‘ headingig.
    """
    lines = (md or "").splitlines()
    sections: List[Tuple[int, str, List[str]]] = []

    current_level = 0
    current_title = ""
    current_buf: List[str] = []

    def flush():
        nonlocal current_level, current_title, current_buf
        if current_level > 0:
            sections.append((current_level, current_title, current_buf))
        current_level = 0
        current_title = ""
        current_buf = []

    for ln in lines:
        m = HEADING_RE.match(ln)
        if m:
            # Ãºj szekciÃ³
            flush()
            current_level = len(m.group(1))
            current_title = m.group(2).strip()
            current_buf = []
        else:
            if current_level == 0:
                # heading elÅ‘tt/utÃ¡n Ã¡llÃ³ tartalom (H1 elÅ‘tti rÃ©sz)
                current_title = ""
                current_level = 0
                current_buf = []
            current_buf.append(ln)

    flush()
    return sections

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PONTOS H2-egyezÃ©shez szÃ¼ksÃ©ges konstansok/fÃ¼ggvÃ©nyek
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

EXACT_VIDEO_HEADING = "VideÃ³ szÃ¶veg"
_VIDEO_TOGGLE_RE = re.compile(
    r"<details>\s*<summary>\s*VideÃ³ szÃ¶veg\s*</summary>\s*(.*?)\s*</details>",
    flags=re.DOTALL | re.IGNORECASE,
)

def _extract_video_toggle(md: str) -> str:
    """
    KizÃ¡rÃ³lag a 'VideÃ³ szÃ¶veg' feliratÃº lenyÃ­lÃ³ (toggle) blokk(ok) tartalmÃ¡t adja vissza.
    Ha tÃ¶bb ilyen blokk van, mindet sorban Ã¶sszefÅ±zi (kettÅ‘s sortÃ¶rÃ©ssel).
    Ha nincs ilyen blokk vagy Ã¼res, Ã¼res stringet ad vissza.
    """
    md = md or ""
    parts = [m.strip() for m in _VIDEO_TOGGLE_RE.findall(md) if m and m.strip()]
    if not parts:
        return ""
    return "\n\n".join(parts)

def choose_section_exact(md: str) -> Tuple[str, str, str]:
    """
    Csak a 'VideÃ³ szÃ¶veg' lenyÃ­lÃ³ blokk tartalmÃ¡t vÃ¡lasztja ki.
    Vissza: (selected_section, raw_text, selected_heading)
    """
    video = _extract_video_toggle(md)
    if video:
        return "video", video, EXACT_VIDEO_HEADING
    return "none", "", ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Markdown tisztÃ­tÃ¡s
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def clean_markdown(md: str) -> str:
    if not md:
        return ""
    # headingek elÅ‘tt 1 Ã¼res sor, kÃ³dblokkok megkÃ­mÃ©lÃ©se, Ã¼res sorok normalizÃ¡lÃ¡sa
    out = []
    in_code = False
    fence = re.compile(r"^\s*```")
    prev_blank = True
    for line in md.splitlines():
        if fence.match(line):
            in_code = not in_code
            out.append(line)
            continue
        if in_code:
            out.append(line)
            continue
        if HEADING_RE.match(line):
            if not prev_blank:
                out.append("")
            out.append(line)
            prev_blank = False
            continue
        if line.strip() == "":
            if not prev_blank:
                out.append("")
                prev_blank = True
            continue
        out.append(line)
        prev_blank = False
    return "\n".join(out).strip()


def renumber_ordered_lists(md: str) -> str:
    """
    SzÃ¡mozott listÃ¡k ÃºjraszÃ¡mozÃ¡sa (kÃ³dblokkokon kÃ­vÃ¼l), '1.' formÃ¡tum tÃ¡mogatott, behÃºzÃ¡s-alapÃº szintek.
    """
    if not md:
        return ""
    lines = md.splitlines()
    out = []
    in_code = False
    fence = re.compile(r"^\s*```")
    list_item = re.compile(r"^(\s*)(\d+)\.\s+")
    counters: Dict[int, int] = {}  # indent â†’ counter

    def level_of(indent: str) -> int:
        return len(indent.replace("\t", "    ")) // 2  # 2 space szint

    for line in lines:
        if fence.match(line):
            in_code = not in_code
            out.append(line)
            continue
        if in_code:
            out.append(line)
            continue

        m = list_item.match(line)
        if m:
            indent = m.group(1)
            lvl = level_of(indent)
            if lvl not in counters:
                counters[lvl] = 1
            else:
                counters[lvl] += 1
            # nullÃ¡zÃ¡s mÃ©lyebb szinteken
            for k in list(counters.keys()):
                if k > lvl:
                    del counters[k]
            newnum = counters[lvl]
            # FIX: \1 helyett \g<1>, hogy ne legyen \11, \110 stb. csoport hivatkozÃ¡s
            line = list_item.sub(r"\g<1>{0}. ".format(newnum), line, count=1)
            out.append(line)
        else:
            out.append(line)
    return "\n".Join(out).strip() if False else "\n".join(out).strip()  # vÃ©dÅ‘hack: ne tÃ¶rÃ¶ld a sort


def enhance_readability(md: str) -> str:
    """
    EgyszerÅ±sÃ­tett formÃ¡zÃ¡s a jobb Ã¡ttekinthetÅ‘sÃ©ghez:
    - egysÃ©ges "- " jelÃ¶lÃ©s a felsorolÃ¡soknÃ¡l,
    - Ã¼res sor beillesztÃ©se listÃ¡k Ã©s cÃ­msorok elÃ©,
    - a cÃ­msorok utÃ¡n egy Ã¼res sort hagy, hogy elkÃ¼lÃ¶nÃ¼ljenek.
    """
    if not md:
        return ""

    lines = md.splitlines()
    out: List[str] = []

    ul_re = re.compile(r"^(\s*)[-*+]\s+(.*)$")
    ol_re = re.compile(r"^(\s*)\d+\.\s+(.*)$")

    for i, line in enumerate(lines):
        heading = HEADING_RE.match(line)
        ul = ul_re.match(line)
        ol = ol_re.match(line)

        if heading:
            if out and out[-1] != "":
                out.append("")
            out.append(line.rstrip())
            out.append("")
            continue

        if ul:
            indent, rest = ul.groups()
            if out and out[-1] != "":
                out.append("")
            out.append(f"{indent}- {rest.strip()}")
            continue

        if ol:
            indent, rest = ol.groups()
            if out and out[-1] != "":
                out.append("")
            out.append(f"{indent}1. {rest.strip()}")
            continue

        if line.strip() == "":
            if out and out[-1] == "":
                continue
            out.append("")
        else:
            out.append(line.rstrip())

    while out and out[-1] == "":
        out.pop()

    return "\n".join(out)

def strip_bold_emphasis(md: str) -> str:
    """
    EltÃ¡volÃ­tja a **â€¦** Ã©s __â€¦__ kiemelÃ©st kÃ³dblokkokon kÃ­vÃ¼l,
    a tartalmat meghagyva (gÃ©pi feldolgozÃ¡st segÃ­ti).
    """
    if not md:
        return ""
    lines = md.splitlines()
    out = []
    in_code = False
    fence = re.compile(r"^\s*```")
    bold_ast = re.compile(r"(?<!\*)\*\*(.+?)\*\*(?!\*)")
    bold_uscr = re.compile(r"(?<!_)__(.+?)__(?!_)")
    for line in lines:
        if fence.match(line):
            in_code = not in_code
            out.append(line)
            continue
        if in_code:
            out.append(line)
            continue
        # inline code vÃ©delme: daraboljuk backtick alapjÃ¡n
        parts = re.split(r"(`[^`]*`)", line)
        for i, part in enumerate(parts):
            if i % 2 == 0:  # nem inline code
                part = bold_ast.sub(r"\1", part)
                part = bold_uscr.sub(r"\1", part)
            out.append(part)
        # join nÃ©lkÃ¼l, mert mÃ¡r out-hoz appendeltÃ¼k rÃ©szenkÃ©nt
        out.append("")  # sorzÃ¡rÃ¡s
    # a fenti extra Ã¼res sorok eltÃ¡volÃ­tÃ¡sa
    out = [ln for ln in out if ln != ""]
    return "\n".join(out).strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# TÃ¡blÃ¡zat kinyerÃ©s
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ALIGN_RE = re.compile(r"^\s*\|?\s*:?-{3,}:?\s*(\|\s*:?-{3,}:?\s*)+\|?\s*$")

def _split_md_row(row: str) -> List[str]:
    # '|' szeparÃ¡lÃ¡s, escape-elt \| figyelembevÃ©tele
    toks = re.split(r"(?<!\\)\|", row.strip().strip("|"))
    toks = [t.replace(r"\|", "|").strip() for t in toks]
    return toks

def _make_key_like(s: str) -> str:
    s = s.strip().lower()
    s = "".join(ch for ch in unicodedata.normalize("NFKD", s) if not unicodedata.combining(ch))
    s = re.sub(r"[^\w]+", "_", s).strip("_")
    return s or "col"

def extract_tables(md: str) -> Tuple[str, List[Dict]]:
    """
    Kinyeri a GFM tÃ¡blÃ¡kat a markdownbÃ³l Ã©s JSON-osÃ­tja.
    Vissza: (md_with_tables_json_section, tables_list)
    """
    if not md:
        return md, []

    lines = md.splitlines()
    tables = []
    i = 0
    while i < len(lines) - 1:
        header_line = lines[i]
        sep_line = lines[i + 1]
        if "|" in header_line and ALIGN_RE.match(sep_line or ""):
            headers = _split_md_row(header_line)
            # adat sorok
            rows = []
            j = i + 2
            while j < len(lines):
                ln = lines[j]
                if "|" not in ln or (HEADING_RE.match(ln) or ln.strip() == ""):
                    break
                rows.append(_split_md_row(ln))
                j += 1
            if headers and rows:
                clean_headers = [strip_bold_emphasis(h) for h in headers]
                keys = [_make_key_like(h) for h in clean_headers]
                row_objs = []
                for r in rows:
                    if len(r) < len(keys):
                        r = r + [""] * (len(keys) - len(r))
                    elif len(r) > len(keys):
                        r = r[:len(keys)]
                    vals = [strip_bold_emphasis(c) for c in r]
                    row_objs.append({k: v for k, v in zip(keys, vals)})
                tables.append({
                    "headers_raw": clean_headers,
                    "headers": keys,
                    "rows": row_objs,
                    "start": i,
                    "end": j - 1,
                })
                i = j
                continue
        i += 1

    if tables:
        out = [md.strip(), "", "## AdattÃ¡blÃ¡k (gÃ©pi kivonat)", ""]
        for idx, t in enumerate(tables, start=1):
            out.append(f"**TÃ¡blÃ¡zat {idx}**")
            out.append("")
            out.append("```json")
            out.append(json.dumps(t, ensure_ascii=False, indent=2))
            out.append("```")
            out.append("")
        return "\n".join(out).strip(), tables
    return md, []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ZIP fÃ¡jlnÃ©v-dekÃ³dolÃ¡s + tartalom (UTF-8 + BOM tÃ¡mogatÃ¡s)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _fixed_zip_filename(info: zipfile.ZipInfo) -> str:
    """
    Ha nincs UTF-8 flag a ZIP fejlÃ©cben, a python zipfile cp437-t feltÃ©telez.
    VisszaalakÃ­tjuk a nevet cp437â†’bytesâ†’utf-8, Ã­gy az Ã©kezetek helyreÃ¡llnak.
    """
    name = info.filename
    try:
        if not (info.flag_bits & 0x800):  # bit11 jelzi az UTF-8 flaget
            return name.encode("cp437").decode("utf-8")
    except Exception:
        pass
    return name

def iter_markdown_files(zf: zipfile.ZipFile) -> List[Tuple[str, str]]:
    out: List[Tuple[str, str]] = []
    for info in zf.infolist():
        if info.is_dir():
            continue
        name = _fixed_zip_filename(info)
        if not name.lower().endswith(".md"):
            continue
        try:
            raw = zf.read(info)
            # BOM tolerant
            try:
                txt = raw.decode("utf-8-sig")
            except UnicodeDecodeError:
                txt = raw.decode("utf-8", errors="replace")
            out.append((name, txt))
        except Exception:
            continue
    return out

def extract_page_id_from_filename(filename: str) -> Optional[str]:
    # Notion export fÃ¡jlnevek vÃ©gÃ©n gyakran ott a 32 hex page_id
    m = re.search(r"([0-9a-f]{32})\.\w+$", filename)
    if m:
        return m.group(1)
    return None

def extract_page_title(md: str, fallback: str) -> str:
    # ElsÅ‘ H1 cÃ­m (# ...)
    for line in (md or "").splitlines():
        m = HEADING_RE.match(line)
        if m and len(m.group(1)) == 1:
            return m.group(2).strip()
    return fallback

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Metaadat-parzolÃ¡s (fejlÃ©c utÃ¡ni kulcs: Ã©rtÃ©k sorok)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_META_ALIASES = {
    "szakasz": ["szakasz", "section", "fejezet", "modul"],
    "video_statusz": ["videÃ³ stÃ¡tusz", "video statusz", "videostatusz", "videÃ³ status", "videostatus", "stÃ¡tusz", "statusz", "stÃ¡tus", "status"],
    "lecke_hossza": ["lecke hossza", "lesson length", "hossz"],
    "utolso_modositas": ["utolsÃ³ mÃ³dosÃ­tÃ¡s", "utolso modositas", "last modified", "utolsÃ³ mÃ³dosÃ­tÃ¡s dÃ¡tuma"],
    "tipus": ["tÃ­pus", "tipus", "type"],
    "kurzus": ["kurzus", "course"],
    "vimeo_link": ["vimeo link", "vimeo url", "vimeo", "videÃ³ url", "video url"],
    "sorszam": ["sorszÃ¡m", "sorszam", "order", "index", "rank"],
}

def _canon_key(raw_key: str) -> Optional[str]:
    nk = normalize(raw_key)
    for can, alist in _META_ALIASES.items():
        for a in alist:
            if normalize(a) == nk:
                return can
    return None

META_LINE_RE = re.compile(r"^\s*([^\:]{1,120})\s*:\s*(.+?)\s*$")

def parse_metadata_block(full_md: str) -> Dict[str, Optional[str]]:
    """
    A H1 utÃ¡n, a kÃ¶vetkezÅ‘ H2-ig terjedÅ‘ blokkban keresi a
    'Kulcs: Ã©rtÃ©k' sorokat.
    Vissza: meta dict kanonikus kulcsokkal (stringek), 'sorszam' â†’ int-kÃ©nt is parse-olhatÃ³.
    """
    lines = (full_md or "").splitlines()
    i = 0
    # Ugorjunk az elsÅ‘ H1 utÃ¡nra
    while i < len(lines):
        m = HEADING_RE.match(lines[i])
        if m and len(m.group(1)) == 1:
            i += 1
            break
        i += 1

    meta: Dict[str, Optional[str]] = {}
    while i < len(lines):
        ln = lines[i].rstrip("\n")
        m = HEADING_RE.match(ln)
        if m and len(m.group(1)) >= 2:
            break
        if not ln.strip():
            i += 1
            continue
        m = META_LINE_RE.match(ln)
        if m:
            raw_k = m.group(1).strip()
            val = m.group(2).strip()
            ck = _canon_key(raw_k)
            if ck:
                meta[ck] = val
        i += 1
    return meta

def meta_sorszam_as_int(meta: Dict[str, Optional[str]]) -> Optional[int]:
    v = (meta.get("sorszam") or "").strip()
    if not v:
        return None
    m = re.search(r"\d+", v.replace(" ", ""))
    if not m:
        return None
    try:
        return int(m.group(0))
    except Exception:
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ChunkolÃ¡s (bekezdÃ©s-hatÃ¡rok mentÃ©n)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def split_by_paragraph(text: str) -> List[str]:
    # KÃ³dblokkokat nem tÃ¶rjÃ¼k meg
    out: List[str] = []
    in_code = False
    fence = re.compile(r"^\s*```")
    buf: List[str] = []
    for ln in (text or "").splitlines():
        if fence.match(ln):
            in_code = not in_code
            buf.append(ln)
            continue
        if in_code:
            buf.append(ln)
            continue
        if ln.strip() == "":
            if buf:
                out.append("\n".join(buf).strip())
                buf = []
        else:
            buf.append(ln)
    if buf:
        out.append("\n".join(buf).strip())
    return [p for p in out if p.strip()]

def chunk_markdown(text: str, target_chars: int = 5500, overlap_chars: int = 400) -> List[Dict]:
    parts = split_by_paragraph(text)
    if not parts:
        return [{"text": text, "start": 0, "end": len(text)}]

    chunks: List[Dict] = []
    cur: List[str] = []
    cur_len = 0
    start = 0
    for p in parts:
        pl = len(p) + 1  # +1: bekezdÃ©s kÃ¶zti \n
        if cur_len + pl > target_chars and cur:
            s = "\n\n".join(cur).strip()
            chunks.append({"text": s, "start": start, "end": start + len(s)})
            # Ã¡tfedÃ©shez: vÃ¡gjuk vissza a vÃ©gÃ©t
            if overlap_chars > 0:
                overlap = s[-overlap_chars:]
                cur = [overlap]
                cur_len = len(overlap)
                start = start + len(s) - overlap_chars
            else:
                cur = []
                cur_len = 0
                start = start + len(s)
        cur.append(p)
        cur_len += pl
    if cur:
        s = "\n\n".join(cur).strip()
        chunks.append({"text": s, "start": start, "end": start + len(s)})
    return chunks

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# KonverziÃ³ (fÅ‘ logika) â€“ JSONL + CSV + Report + Clean MD + Tables JSONL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def convert_zip_to_datasets(
    zip_bytes: bytes,
    do_chunk: bool,
    target_chars: int,
    overlap_chars: int
) -> Tuple[bytes, bytes, bytes, bytes, bytes]:
    """
    Vissza: (jsonl_bytes, csv_bytes_bom, report_csv_bytes_bom, clean_md_zip_bytes, tables_jsonl_bytes)
    """
    rid = run_id()
    zf = zipfile.ZipFile(io.BytesIO(zip_bytes), "r")
    md_files = iter_markdown_files(zf)

    jsonl_buf = io.StringIO()

    # CSV bufferek: Windows/Excel kompatibilis sorvÃ©gekkel
    csv_buf = io.StringIO(newline="")
    rep_buf = io.StringIO(newline="")
    csv_w = csv.writer(csv_buf, lineterminator="\n")
    rep_w = csv.writer(rep_buf, lineterminator="\n")

    # CSV fejlÃ©c â€“ kiegÃ©szÃ­tve meta oszlopokkal
    csv_w.writerow([
        "file_name", "page_id", "page_title",
        "selected_section", "selected_heading",
        "char_len", "tartalom",
        "meta_szakasz", "meta_video_statusz", "meta_lecke_hossza", "meta_utolso_modositas",
        "meta_tipus", "meta_kurzus", "meta_vimeo_link", "meta_sorszam"
    ])
    rep_w.writerow(["file_name", "page_id", "page_title", "video_len", "lesson_len", "selected", "selected_len"])

    # TisztÃ­tott MD-k kÃ¼lÃ¶n ZIP-be
    md_zip_buf = io.BytesIO()
    md_zip = zipfile.ZipFile(md_zip_buf, "w", compression=zipfile.ZIP_DEFLATED)
    used_names = set()  # â† Ã¼tkÃ¶zÃ©skezelÃ©s a ZIP-ben

    # TÃ¡blÃ¡zatok kÃ¼lÃ¶n JSONL-be (Ã¶sszes dokumentum)
    tables_jsonl_buf = io.StringIO()

    total = len(md_files)
    ok = 0
    skipped = 0  # nincs szÅ±rÃ©s, nem nÅ‘
    progress = st.progress(0.0, text=f"0/{total} feldolgozva (âœ…: 0, kihagyva: 0)")

    for idx, (fname, text) in enumerate(md_files, start=1):
        page_id = extract_page_id_from_filename(fname) or ""
        title = extract_page_title(text, fallback=os.path.splitext(os.path.basename(fname))[0])

        # Metaadatok a H1 utÃ¡ni blokkbÃ³l
        meta = parse_metadata_block(text)
        sorsz_int = meta_sorszam_as_int(meta)

        # LenyÃ­lÃ³ (toggle) VideÃ³ szÃ¶veg blokk kinyerÃ©se
        video_txt = _extract_video_toggle(text)

        # KivÃ¡lasztÃ¡s: csak a lenyÃ­lÃ³ VideÃ³ szÃ¶veg tartalma szÃ¡mÃ­t
        selected, raw, selected_heading = choose_section_exact(text)

        # tisztÃ­tÃ¡s
        raw_clean = strip_bold_emphasis(raw)
        raw_clean = clean_markdown(raw_clean)
        raw_clean = enhance_readability(raw_clean)
        raw_clean = renumber_ordered_lists(raw_clean)

        # tÃ¡blÃ¡zatok kivonata csak a kivÃ¡lasztott szÃ¶vegbÅ‘l
        md_with_tables, tables = extract_tables(raw_clean if raw_clean else "")
        if tables:
            # tÃ¡blÃ¡k JSONL â€“ globÃ¡lis gyÅ±jtÅ‘
            for t in tables:
                tables_jsonl_buf.write(json.dumps({
                    "run_id": rid,
                    "page_id": page_id,
                    "file_name": os.path.basename(fname),
                    "page_title": title,
                    "selected_section": selected,
                    "selected_heading": selected_heading,
                    "table": t
                }, ensure_ascii=False) + "\n")

        # JSONL rekord(ok)
        base_rec = {
            "run_id": rid,
            "doc_id": slugify(title) if not page_id else f"{slugify(title)}_{page_id[:8]}",
            "page_id": page_id,
            "file_name": os.path.basename(fname),
            "page_title": title,
            "selected_section": selected,
            "selected_heading": selected_heading,
            "char_len": len(md_with_tables),
            # meta:
            "meta_szakasz": meta.get("szakasz") or "",
            "meta_video_statusz": meta.get("video_statusz") or "",
            "meta_lecke_hossza": meta.get("lecke_hossza") or "",
            "meta_utolso_modositas": meta.get("utolso_modositas") or "",
            "meta_tipus": meta.get("tipus") or "",
            "meta_kurzus": meta.get("kurzus") or "",
            "meta_vimeo_link": meta.get("vimeo_link") or "",
            "meta_sorszam": sorsz_int if sorsz_int is not None else "",
        }

        if do_chunk:
            try:
                parts = chunk_markdown(md_with_tables, target_chars, overlap_chars)
            except Exception as e:
                st.warning(f"ChunkolÃ¡s kÃ¶zbeni hiba: {e}. Teljes szÃ¶veg egy blokkban mentve.")
                parts = [{"text": md_with_tables, "start": 0, "end": len(md_with_tables)}]
            for i, ch in enumerate(parts, start=1):
                rec = dict(base_rec)
                rec.update({"chunk_index": i, "text_markdown": ch["text"], "char_len": len(ch["text"])})
                jsonl_buf.write(json.dumps(rec, ensure_ascii=False) + "\n")
        else:
            rec = dict(base_rec)
            rec.update({"text_markdown": md_with_tables})
            jsonl_buf.write(json.dumps(rec, ensure_ascii=False) + "\n")

        # CSV sor
        csv_w.writerow([
            os.path.basename(fname),
            page_id,
            title,
            selected,
            selected_heading,
            len(md_with_tables),
            md_with_tables,
            base_rec["meta_szakasz"], base_rec["meta_video_statusz"], base_rec["meta_lecke_hossza"],
            base_rec["meta_utolso_modositas"], base_rec["meta_tipus"], base_rec["meta_kurzus"],
            base_rec["meta_vimeo_link"], base_rec["meta_sorszam"]
        ])

        # Riport
        rep_w.writerow([
            os.path.basename(fname),
            page_id,
            title,
            len(video_txt),
            0,
            selected,
            len(md_with_tables)
        ])

        # â”€â”€ TisztÃ­tott MD kÃ©szÃ­tÃ©se meta blokkal a H1 utÃ¡n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        md_name_base = build_md_filename(title, sorsz_int, page_id, meta.get("kurzus") or "")
        md_name = uniquify_filename(md_name_base, used_names, page_id)

        # Meta cÃ­mkÃ©k megjelenÃ­tÃ©si sorrendben
        meta_labels = [
            ("Szakasz", "szakasz"),
            ("VideÃ³ stÃ¡tusz", "video_statusz"),
            ("Lecke hossza", "lecke_hossza"),
            ("UtolsÃ³ mÃ³dosÃ­tÃ¡s", "utolso_modositas"),
            ("TÃ­pus", "tipus"),
            ("Kurzus", "kurzus"),
            ("Vimeo link", "vimeo_link"),
        ]
        meta_lines = []
        for label, key in meta_labels:
            val = (meta.get(key) or "").strip()
            if val:
                meta_lines.append(f"{label}: {val}")

        md_lines = []
        if title:
            md_lines.append(f"# {title}")
        if meta_lines:
            md_lines.append("\n".join(meta_lines))  # meta blokk
        if selected_heading:
            md_lines.append(f"## {selected_heading}")
        if md_with_tables.strip():
            md_lines.append(md_with_tables)
        else:
            md_lines.append("Ehhez a leckÃ©hez nem kÃ©szÃ¼lt leÃ­rÃ¡s.")
        clean_md_text = "\n\n".join([ln for ln in md_lines if ln]).strip()

        md_zip.writestr(md_name, clean_md_text.encode("utf-8"))

        ok += 1
        pct = idx / max(1, total)
        progress.progress(pct, text=f"{idx}/{total} feldolgozva (âœ…: {ok}, kihagyva: {skipped})")

    # ZÃ¡rÃ¡sok Ã©s kimenetek elÅ‘Ã¡llÃ­tÃ¡sa
    md_zip.close()
    clean_md_zip_bytes = md_zip_buf.getvalue()

    jsonl_bytes = (jsonl_buf.getvalue()).encode("utf-8")
    csv_bytes_bom = ("\ufeff" + csv_buf.getvalue()).encode("utf-8")     # BOM
    rep_bytes_bom = ("\ufeff" + rep_buf.getvalue()).encode("utf-8")     # BOM
    tables_jsonl_bytes = (tables_jsonl_buf.getvalue()).encode("utf-8")

    return jsonl_bytes, csv_bytes_bom, rep_bytes_bom, clean_md_zip_bytes, tables_jsonl_bytes


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with st.expander("Mi ez?"):
    st.markdown(
        "- TÃ¶lts fel egy **Notion export ZIP**-et (Markdown & CSV exportbÃ³l a ZIP-et hasznÃ¡ld).\n"
        "- A konverter az Ã¶sszes `VideÃ³ szÃ¶veg` lenyÃ­lÃ³ (toggle) blokk teljes tartalmÃ¡t veszi ki.\n"
        "- Ha nincs ilyen lenyÃ­lÃ³ blokk, a kimenet: _Ehhez a leckÃ©hez nem kÃ©szÃ¼lt leÃ­rÃ¡s._\n"
        "- A fÃ©lkÃ¶vÃ©r (**â€¦**) jelÃ¶lÃ©st eltÃ¡volÃ­tja (kÃ³dblokkok Ã©rintetlenek), a cÃ­msorokat Ã©s listÃ¡kat jobban tagolja az olvashatÃ³sÃ¡gÃ©rt.\n"
        "- A tÃ¡blÃ¡zatokat (GFM) felismeri Ã©s **JSON kivonatot** kÃ©szÃ­t rÃ³luk.\n"
        "- **Metaadatok megÅ‘rzÃ©se**: a *Szakasz, VideÃ³ stÃ¡tusz, Lecke hossza, UtolsÃ³ mÃ³dosÃ­tÃ¡s, TÃ­pus, Kurzus, Vimeo link* sorok a H1 utÃ¡n bekerÃ¼lnek a tisztÃ­tott MD-be.\n"
        "- A tisztÃ­tott MD fÃ¡jlnÃ©v sÃ©mÃ¡ja: `Kurzus - SorszÃ¡m - NÃ©v.md`.\n"
        "- Kimenet: **tisztÃ­tott MD-k (ajÃ¡nlott)** + haladÃ³ formÃ¡tumok: JSONL, CSV, riport CSV, tÃ¡blÃ¡zatok JSONL.\n"
        "- OpcionÃ¡lis: **chunkolÃ¡s** Ã¡tfedÃ©ssel (JSONL-hoz)."
    )

st.sidebar.header("BeÃ¡llÃ­tÃ¡sok")
do_chunk = st.sidebar.checkbox("JSONL chunkolÃ¡sa", value=True)
target_chars = st.sidebar.number_input("Chunk cÃ©lszÃ©lessÃ©g (karakter)", min_value=1000, max_value=20000, value=5500, step=500)
overlap_chars = st.sidebar.number_input("Chunk Ã¡tfedÃ©s (karakter)", min_value=0, max_value=5000, value=400, step=50)

uploaded = st.file_uploader("TÃ¶ltsd fel a Notion Markdown ZIP-et", type=["zip"])

if uploaded is not None:
    try:
        b = uploaded.read()
        jsonl_bytes, csv_bytes_bom, rep_bytes_bom, md_zip_bytes, tables_jsonl_bytes = convert_zip_to_datasets(
            b, do_chunk, target_chars, overlap_chars
        )
    except zipfile.BadZipFile:
        st.error("HibÃ¡s ZIP fÃ¡jl.")
        st.stop()
    except Exception as e:
        st.error(f"VÃ¡ratlan hiba: {e}")
        st.stop()

    rid = run_id()

    st.success("KÃ©sz! VÃ¡laszd ki a letÃ¶ltÃ©st.")

    # ElsÅ‘dleges letÃ¶ltÃ©s: tisztÃ­tott MD-k
    st.download_button(
        "â¬‡ï¸ TisztÃ­tott MD-k (ZIP)",
        data=md_zip_bytes,
        file_name=f"clean_md_{rid}.zip",
        mime="application/zip",
        use_container_width=True
    )

    st.divider()

    # â”€â”€ MÃ¡sodlagos / haladÃ³ formÃ¡tumok: expanderben â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("HaladÃ³ letÃ¶ltÃ©sek (JSONL/CSV/riport/tÃ¡blÃ¡zatok/Minden egyben)", expanded=False):
        c1, c2, c3 = st.columns(3)
        with c1:
            st.download_button(
                "â¬‡ï¸ JSONL (szÃ¶veg, RAG/finetune)",
                data=jsonl_bytes,
                file_name=f"output_{rid}.jsonl",
                mime="application/json",
                use_container_width=True
            )
        with c2:
            st.download_button(
                "â¬‡ï¸ CSV (Excel-barÃ¡t, BOM)",
                data=csv_bytes_bom,
                file_name=f"output_{rid}.csv",
                mime="text/csv",
                use_container_width=True
            )
        with c3:
            st.download_button(
                "â¬‡ï¸ Riport CSV",
                data=rep_bytes_bom,
                file_name=f"report_{rid}.csv",
                mime="text/csv",
                use_container_width=True
            )

        st.download_button(
            "â¬‡ï¸ TÃ¡blÃ¡zatok (JSONL)",
            data=tables_jsonl_bytes,
            file_name=f"tables_{rid}.jsonl",
            mime="application/json",
            use_container_width=True
        )

        # MINDEN EGYBEN ZIP
        with io.BytesIO() as all_buf:
            with zipfile.ZipFile(all_buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                zf.writestr(f"output_{rid}.jsonl", jsonl_bytes)
                zf.writestr(f"output_{rid}.csv", csv_bytes_bom)
                zf.writestr(f"report_{rid}.csv", rep_bytes_bom)
                zf.writestr(f"tables_{rid}.jsonl", tables_jsonl_bytes)
                # a tisztÃ­tott MD ZIP tartalmÃ¡t al-mappakÃ©nt bepakoljuk
                with zipfile.ZipFile(io.BytesIO(md_zip_bytes), "r") as mdzf:
                    for info in mdzf.infolist():
                        data = mdzf.read(info.filename)
                        zf.writestr(f"clean_md/{info.filename}", data)
            all_buf.seek(0)

            st.download_button(
                "â¬‡ï¸ Minden egyben (ZIP)",
                data=all_buf.getvalue(),
                file_name=f"converted_{rid}.zip",
                mime="application/zip",
                use_container_width=True
            )
