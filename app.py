import io
import os
import re
import csv
import json
import time
import zipfile
import unicodedata
from datetime import datetime
from typing import List, Dict, Tuple, Optional

import streamlit as st

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Streamlit config
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.set_page_config(page_title="Notion MD ‚Üí ChatGPT", page_icon="üß©", layout="wide")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Helpers: run_id, normalize (csak bels≈ë matchinghez), slug, safe filename
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run_id() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def normalize(s: str) -> str:
    """√âkezet/√≠r√°sjel-agnosztikus √∂sszehasonl√≠t√°shoz (bels≈ë matching)."""
    if s is None:
        return ""
    s = unicodedata.normalize("NFKD", s)
    s = s.encode("ascii", "ignore").decode("ascii")
    s = s.strip().lower()
    s = re.sub(r"[^\w\s]", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s

def slugify(s: str, maxlen: int = 100) -> str:
    s = normalize(s).replace(" ", "_")
    s = re.sub(r"[^a-z0-9_]+", "", s)
    return s[:maxlen] if len(s) > maxlen else s

def safe_filename(title: str, page_id: Optional[str] = None, maxlen: int = 140) -> str:
    """√âkezeteket meghagyjuk; csak tiltott f√°jlneveket cser√©l√ºnk."""
    base = title.strip() if title else "untitled"
    base = re.sub(r'[\\/:*?"<>|]+', "_", base)
    base = re.sub(r"\s+", " ", base).strip()
    if page_id:
        base = f"{base} {page_id}"
    if len(base) > maxlen:
        base = base[:maxlen].rstrip()
    if not base:
        base = page_id or "untitled"
    return base + ".md"

def extract_page_id_from_filename(name: str) -> Optional[str]:
    """'C√≠m abcdef1234567890abcdef1234567890.md' ‚Üí 32 hex azonos√≠t√≥"""
    base = os.path.splitext(os.path.basename(name))[0]
    m = re.search(r"([0-9a-fA-F]{32})$", base)
    return m.group(1).lower() if m else None

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Markdown ‚Üí szekci√≥k
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
HEADING_RE = re.compile(r'^(#+)\s+(.*)$')

def split_markdown_sections(md: str) -> List[Tuple[int, str, List[str]]]:
    """A Markdown-t heading-alap√∫ szekci√≥kra bontja. Vissza: (level, title, lines)."""
    lines = (md or "").splitlines()
    sections: List[Tuple[int, str, List[str]]] = []
    current_level = None
    current_title = None
    current_buf: List[str] = []

    def flush():
        nonlocal current_level, current_title, current_buf
        if current_title is not None:
            sections.append((current_level or 0, current_title, current_buf))
        current_level, current_title, current_buf = None, None, []

    for ln in lines:
        m = HEADING_RE.match(ln)
        if m:
            flush()
            level = len(m.group(1))
            title = m.group(2).strip()
            current_level, current_title = level, title
            current_buf = []
        else:
            if current_title is None:
                current_title = ""
                current_level = 0
                current_buf = []
            current_buf.append(ln)

    flush()
    return sections

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# C√©l c√≠mk√©k (konfigur√°lhat√≥ a UI-ban)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
DEFAULT_VIDEO_LABELS = [
    "vide√≥ sz√∂veg", "video szoveg", "vide√≥ leirat", "video leirat",
    "transcript", "vide√≥", "video",
]
DEFAULT_LESSON_LABELS = [
    "lecke sz√∂veg", "lecke anyag", "leckesz√∂veg", "tananyag",
]

def label_match(title: str, target_tokens: List[str]) -> bool:
    """Fuzzy/normaliz√°lt egyez√©s (√©kezet, √≠r√°sjelek elhagy√°sa; token-alap√∫ r√©szleges match)."""
    tnorm = normalize(title)
    for raw in target_tokens:
        cand = normalize(raw)
        subtoks = cand.split()
        ok = all(tok in tnorm for tok in subtoks if tok)
        if ok:
            return True
    return False

def choose_section(
    sections: List[Tuple[int, str, List[str]]],
    video_labels: List[str],
    lesson_labels: List[str],
    min_level: int = 2,
    max_level: int = 4
) -> Tuple[str, str, str]:
    """
    Kiv√°laszt√°s szab√°ly szerint:
      1) Vide√≥-c√≠mk√©s H2‚ÄìH4 nem √ºres ‚Üí 'video'
      2) k√ºl√∂nben Lecke-c√≠mk√©s H2‚ÄìH4 nem √ºres ‚Üí 'lecke'
      3) k√ºl√∂nben 'none'
    Vissza: (selected_section, text_markdown, selected_heading).
    """
    candidates: Dict[str, List[Tuple[int,str,List[str]]]] = {"video": [], "lecke": []}
    for level, title, lines in sections:
        if min_level <= level <= max_level:
            if label_match(title, video_labels):
                candidates["video"].append((level, title, lines))
            elif label_match(title, lesson_labels):
                candidates["lecke"].append((level, title, lines))

    def text_of(sec) -> str:
        return "\n".join(sec[2]).strip()

    for sec in candidates["video"]:
        txt = text_of(sec)
        if txt:
            return "video", txt, sec[1]
    for sec in candidates["lecke"]:
        txt = text_of(sec)
        if txt:
            return "lecke", txt, sec[1]
    return "none", "", ""

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Markdown tiszt√≠t√°s + list√°k √∫jrasz√°moz√°sa + f√©lk√∂v√©r elt√°vol√≠t√°s
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def clean_markdown(md: str) -> str:
    """K√≠m√©letes tiszt√≠t√°s: √ºres sorok, c√≠msorok el≈ëtti √ºres sor, id√©zetek stb."""
    if not md:
        return ""
    md = re.sub(r"^(#+)([^\s#])", r"\1 \2", md, flags=re.M)     # ###C√≠m -> ### C√≠m
    md = re.sub(r"(\n#+\s)", r"\n\n\1", md)                     # heading el√© √ºres sor
    md = re.sub(r"(\n>\s)", r"\n\n\1", md)                      # id√©zet el√© √ºres sor
    md = re.sub(r"\n{3,}", "\n\n", md)                          # 3+ √ºres sor ‚Üí 1
    md = re.sub(r"^>\s-\s", "- ", md, flags=re.M)               # id√©zet-lista kisim√≠t√°s
    return md.strip()

def renumber_ordered_lists(md: str) -> str:
    """Sz√°mozott list√°k √∫jrasz√°moz√°sa k√≥dblokkokon k√≠v√ºl."""
    if not md:
        return ""
    lines = md.splitlines()
    out: List[str] = []
    in_code = False
    fence = re.compile(r"^\s*```")
    num = re.compile(r"^(\s*)(\d+)\.\s+(.*)$")
    counters: Dict[int, int] = {}
    active_indent: Optional[int] = None
    for line in lines:
        if fence.match(line):
            in_code = not in_code
            out.append(line); continue
        if in_code:
            out.append(line); continue
        m = num.match(line)
        if m:
            indent = len(m.group(1))
            content = m.group(3)
            if active_indent is None or indent != active_indent:
                active_indent = indent
                for k in list(counters.keys()):
                    if k >= indent: del counters[k]
                counters[indent] = 1
            else:
                counters[indent] = counters.get(indent, 0) + 1
            out.append(" " * indent + f"{counters[indent]}. " + content)
        else:
            active_indent = None
            out.append(line)
    return "\n".join(out).strip()

def strip_bold_emphasis(md: str) -> str:
    """
    Elt√°vol√≠tja a **‚Ä¶** √©s __‚Ä¶__ kiemel√©st k√≥dblokkokon k√≠v√ºl,
    a tartalmat meghagyva (g√©pi feldolgoz√°st seg√≠ti).
    """
    if not md:
        return ""
    lines = md.splitlines()
    out = []
    in_code = False
    fence = re.compile(r"^\s*```")
    bold_ast = re.compile(r"(?<!\*)\*\*(.+?)\*\*(?!\*)")
    bold_uscr = re.compile(r"(?<!_)__(.+?)__(?!_)")
    for line in lines:
        if fence.match(line):
            in_code = not in_code
            out.append(line)
            continue
        if in_code:
            out.append(line)
            continue
        # inline code v√©delme: daraboljuk backtick alapj√°n
        parts = re.split(r"(`[^`]*`)", line)
        for i, part in enumerate(parts):
            if i % 2 == 0:  # nem inline code
                part = bold_ast.sub(r"\1", part)
                part = bold_uscr.sub(r"\1", part)
            parts[i] = part
        out.append("".join(parts))
    return "\n".join(out).strip()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# T√°bl√°zat detekt√°l√°s √©s JSON-kivonat
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ALIGN_RE = re.compile(r'^\s*\|?(?:\s*:?-{3,}:?\s*\|)+\s*:?-{3,}:?\s*\|?\s*$')

def _split_md_row(line: str) -> List[str]:
    """Biztons√°gos cella-sz√©tv√°laszt√≥: figyel az escape-elt '|' √©s az inline code-ra."""
    cells, buf = [], []
    in_code = False
    esc = False
    for ch in line.strip():
        if esc:
            buf.append(ch); esc = False; continue
        if ch == '\\':
            esc = True; continue
        if ch == '`':
            in_code = not in_code
            buf.append(ch); continue
        if ch == '|' and not in_code:
            cells.append("".join(buf).strip()); buf = []; continue
        buf.append(ch)
    cells.append("".join(buf).strip())
    # T√°vol√≠tsuk el a kezd≈ë/z√°r√≥ pipe miatti √ºres elemeket
    if cells and cells[0] == "": cells = cells[1:]
    if cells and cells[-1] == "": cells = cells[:-1]
    return cells

def _normalize_key(s: str) -> str:
    s = unicodedata.normalize("NFKD", s).encode("ascii", "ignore").decode("ascii")
    s = s.strip().lower()
    s = re.sub(r"[^\w]+", "_", s).strip("_")
    return s or "col"

def extract_tables(md: str) -> Tuple[str, List[Dict]]:
    """
    Kinyeri a GFM t√°bl√°kat a markdownb√≥l √©s JSON-os√≠tja.
    Vissza: (md_with_tables_json_section, tables_list)
      - md_with_tables_json_section: az eredeti (tiszt√≠tott) md + "Adatt√°bl√°k (g√©pi kivonat)" szekci√≥ k√≥dblokkokkal
      - tables_list: [{headers:[..], rows:[{..},..]}, ...]
    """
    if not md:
        return md, []

    lines = md.splitlines()
    tables = []
    i = 0
    used_sections = []
    while i < len(lines) - 1:
        header_line = lines[i]
        sep_line = lines[i + 1]
        if "|" in header_line and ALIGN_RE.match(sep_line or ""):
            headers = _split_md_row(header_line)
            # adat sorok
            rows = []
            j = i + 2
            while j < len(lines):
                ln = lines[j]
                if "|" not in ln or (HEADING_RE.match(ln) or ln.strip() == ""):
                    break
                rows.append(_split_md_row(ln))
                j += 1
            if headers and rows:
                # f√©lk√∂v√©r tiszt√≠t√°s a cell√°kban
                clean_headers = [strip_bold_emphasis(h) for h in headers]
                # normaliz√°lt kulcsok, √ºtk√∂z√©sek kezel√©se
                keys = []
                seen = set()
                for h in clean_headers:
                    k = _normalize_key(h)
                    if k in seen:
                        c = 2
                        while f"{k}_{c}" in seen:
                            c += 1
                        k = f"{k}_{c}"
                    keys.append(k); seen.add(k)
                row_objs = []
                for r in rows:
                    # kiegyenl√≠t√©s (r√∂videbb/hosszabb sorok)
                    if len(r) < len(keys):
                        r = r + [""] * (len(keys) - len(r))
                    elif len(r) > len(keys):
                        r = r[:len(keys)]
                    vals = [strip_bold_emphasis(c) for c in r]
                    row_objs.append({k: v for k, v in zip(keys, vals)})
                tables.append({
                    "headers_raw": clean_headers,
                    "headers": keys,
                    "rows": row_objs,
                    "start": i,
                    "end": j - 1,
                })
                used_sections.append((i, j - 1))
                i = j
                continue
        i += 1

    if not tables:
        return md, []

    # Az MD v√©g√©re illeszt√ºnk egy √∂sszefoglal√≥ szekci√≥t JSON-okkal
    out = [md, "", "## Adatt√°bl√°k (g√©pi kivonat)", ""]
    for idx, t in enumerate(tables, start=1):
        out.append(f"### T√°bl√°zat {idx}")
        out.append("```json")
        out.append(json.dumps({"headers": t["headers"], "rows": t["rows"]}, ensure_ascii=False, indent=2))
        out.append("```")
        out.append("")
    return "\n".join(out).strip(), tables

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ZIP f√°jln√©v-dek√≥dol√°s + tartalom (UTF-8 + BOM t√°mogat√°s)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _fixed_zip_filename(info: zipfile.ZipInfo) -> str:
    """
    Ha nincs UTF-8 flag a ZIP fejl√©cben, a python zipfile cp437-t felt√©telez.
    Visszaalak√≠tjuk a nevet cp437‚Üíbytes‚Üíutf-8, √≠gy az √©kezetek helyre√°llnak.
    """
    name = info.filename
    try:
        if not (info.flag_bits & 0x800):  # bit11 jelzi az UTF-8 flaget
            return name.encode("cp437").decode("utf-8")
    except Exception:
        pass
    return name

def iter_markdown_files(zf: zipfile.ZipFile) -> List[Tuple[str, str]]:
    """
    Bej√°rja a ZIP-et, (arcname, text) list√°t ad .md f√°jlokra.
    Tartalom dek√≥dol√°s: 'utf-8-sig' ‚Üí elt√°vol√≠tja a BOM-ot.
    """
    md_items: List[Tuple[str, str]] = []
    for info in zf.infolist():
        if info.is_dir():
            continue
        if not info.filename.lower().endswith(".md"):
            continue
        fixed_name = _fixed_zip_filename(info)
        with zf.open(info, "r") as f:
            b = f.read()
        try:
            s = b.decode("utf-8-sig")
        except UnicodeDecodeError:
            s = b.decode("utf-8", errors="replace")
        md_items.append((fixed_name, s))
    return md_items

def extract_page_title(md: str, fallback: str) -> str:
    # Els≈ë H1 c√≠m (# ...)
    for line in (md or "").splitlines():
        m = HEADING_RE.match(line)
        if m and len(m.group(1)) == 1:
            return m.group(2).strip()
    return fallback

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Konverzi√≥ (f≈ë logika) ‚Äì JSONL + CSV + Report + Clean MD + Tables JSONL
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def convert_zip_to_datasets(
    zip_bytes: bytes,
    video_labels: List[str],
    lesson_labels: List[str],
    chunk: bool,
    target_chars: int,
    overlap_chars: int
) -> Tuple[bytes, bytes, bytes, bytes, bytes]:
    """
    Vissza: (jsonl_bytes, csv_bytes_bom, report_csv_bytes_bom, clean_md_zip_bytes, tables_jsonl_bytes)
    """
    rid = run_id()
    zf = zipfile.ZipFile(io.BytesIO(zip_bytes), "r")
    md_files = iter_markdown_files(zf)

    jsonl_buf = io.StringIO()

    # CSV bufferek: Windows/Excel kompatibilis sorv√©gekkel
    csv_buf = io.StringIO(newline="")
    rep_buf = io.StringIO(newline="")
    csv_w = csv.writer(csv_buf, lineterminator="\n")
    rep_w = csv.writer(rep_buf, lineterminator="\n")

    csv_w.writerow(["file_name", "page_id", "page_title", "selected_section", "selected_heading", "char_len", "tartalom"])
    rep_w.writerow(["file_name", "page_id", "page_title", "video_len", "lesson_len", "selected", "selected_len"])

    # Tiszt√≠tott MD-k k√ºl√∂n ZIP-be
    md_zip_buf = io.BytesIO()
    md_zip = zipfile.ZipFile(md_zip_buf, "w", compression=zipfile.ZIP_DEFLATED)

    # T√°bl√°zatok k√ºl√∂n JSONL-be (√∂sszes dokumentum)
    tables_jsonl_buf = io.StringIO()

    total = len(md_files)
    ok = 0
    progress = st.progress(0.0, text=f"0/{total} feldolgozva")

    for idx, (fname, text) in enumerate(md_files, start=1):
        page_id = extract_page_id_from_filename(fname) or ""
        title = extract_page_title(text, fallback=os.path.splitext(os.path.basename(fname))[0])

        # Sz√©tbont√°s szekci√≥kra √©s v√°laszt√°s
        sections = split_markdown_sections(text)
        video_txt = ""; lesson_txt = ""
        for level, heading, lines in sections:
            if 2 <= level <= 4:
                if label_match(heading, video_labels):  video_txt = "\n".join(lines).strip()
                if label_match(heading, lesson_labels): lesson_txt = "\n".join(lines).strip()

        selected, raw, selected_heading = choose_section(sections, video_labels, lesson_labels, 2, 4)

        # Tiszt√≠t√°s + list√°k + f√©lk√∂v√©r elt√°vol√≠t√°s
        cleaned = renumber_ordered_lists(clean_markdown(raw))
        cleaned = strip_bold_emphasis(cleaned)

        # T√°bl√°k kinyer√©se √©s MD-hez csatol√°sa g√©pi kivonattal
        md_with_tables, tables = extract_tables(cleaned)

        # JSONL / CSV √≠r√°s
        if chunk:
            parts = chunk_markdown(md_with_tables, target_chars, overlap_chars)
            for i, ch in enumerate(parts, start=1):
                rec = {
                    "run_id": rid,
                    "doc_id": f"{slugify(title)}_{(page_id or 'noid')}",
                    "page_id": page_id,
                    "file_name": os.path.basename(fname),
                    "page_title": title,
                    "selected_section": selected,
                    "selected_heading": selected_heading,
                    "chunk_index": i,
                    "text_markdown": ch["text"],
                    "char_len": len(ch["text"]),
                }
                jsonl_buf.write(json.dumps(rec, ensure_ascii=False) + "\n")
        else:
            rec = {
                "run_id": rid,
                "doc_id": f"{slugify(title)}_{(page_id or 'noid')}",
                "page_id": page_id,
                "file_name": os.path.basename(fname),
                "page_title": title,
                "selected_section": selected,
                "selected_heading": selected_heading,
                "text_markdown": md_with_tables,   # a t√°bl√°k g√©pi kivonata is benne a v√©g√©n
                "char_len": len(md_with_tables),
            }
            jsonl_buf.write(json.dumps(rec, ensure_ascii=False) + "\n")

        csv_w.writerow([
            os.path.basename(fname),
            page_id,
            title,
            selected,
            selected_heading,
            len(md_with_tables),
            md_with_tables
        ])

        rep_w.writerow([
            os.path.basename(fname),
            page_id,
            title,
            len(video_txt),
            len(lesson_txt),
            selected,
            len(md_with_tables)
        ])

        # Tiszt√≠tott MD f√°jl (emberbar√°t + v√©g√©n g√©pi kivonat)
        md_name = safe_filename(title, page_id=page_id)
        md_lines = []
        if title: md_lines.append(f"# {title}")
        if selected_heading: md_lines.append(f"## {selected_heading}")
        if md_with_tables.strip(): md_lines.append(md_with_tables.strip())
        md_content = "\n\n".join(md_lines).strip() + "\n"
        md_zip.writestr(f"{md_name}", md_content.encode("utf-8"))

        # T√°bl√°zatok √∂sszegy≈±jt√©se JSONL-be
        for t_index, t in enumerate(tables, start=1):
            tables_jsonl_buf.write(json.dumps({
                "run_id": rid,
                "doc_id": f"{slugify(title)}_{(page_id or 'noid')}",
                "page_id": page_id,
                "file_name": os.path.basename(fname),
                "page_title": title,
                "selected_section": selected,
                "selected_heading": selected_heading,
                "table_index": t_index,
                "headers": t["headers"],
                "rows": t["rows"]
            }, ensure_ascii=False) + "\n")

        ok += 1
        pct = ok / max(1, total)
        progress.progress(pct, text=f"{ok}/{total} feldolgozva")

    # Z√°r√°sok √©s kimenetek el≈ë√°ll√≠t√°sa
    md_zip.close()
    clean_md_zip_bytes = md_zip_buf.getvalue()

    jsonl_bytes = jsonl_buf.getvalue().encode("utf-8")
    tables_jsonl_bytes = tables_jsonl_buf.getvalue().encode("utf-8")

    csv_text = csv_buf.getvalue()
    rep_text = rep_buf.getvalue()

    # UTF-8 BOM a CSV-khez (Excel)
    csv_bytes = ("\ufeff" + csv_text).encode("utf-8")
    rep_bytes = ("\ufeff" + rep_text).encode("utf-8")

    return jsonl_bytes, csv_bytes, rep_bytes, clean_md_zip_bytes, tables_jsonl_bytes

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# UI
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.title("üß© Notion Markdown ‚Üí ChatGPT (JSONL/CSV/MD) konverter")
st.caption("Duplik√°ci√≥k kiz√°r√°sa (Vide√≥‚ÜíLecke), f√©lk√∂v√©r tiszt√≠t√°s, t√°bl√°zatok g√©pi kivonata. UTF-8/√©kezet, CSV BOM-mal.")

with st.expander("Mi ez?"):
    st.markdown(
        "- T√∂lts fel egy **Notion export ZIP**-et (Markdown & CSV exportb√≥l a ZIP-et haszn√°ld).\n"
        "- A konverter a **‚ÄûVide√≥ sz√∂vege‚Äù** (vagy rokon c√≠mke) tartalmat v√°gja ki; ha √ºres, akkor a **‚ÄûLecke sz√∂vege‚Äù**-t.\n"
        "- A f√©lk√∂v√©r (**‚Ä¶**) jel√∂l√©st **elt√°vol√≠tja** a jobb g√©pi feldolgozhat√≥s√°g√©rt.\n"
        "- A t√°bl√°zatokat (GFM) felismeri √©s **JSON kivonatot** is k√©sz√≠t r√≥luk.\n"
        "- Kimenet: **JSONL** (sz√∂veg), **CSV**, **riport CSV**, **tiszt√≠tott MD-k (ZIP)**, **t√°bl√°zatok (JSONL)**.\n"
        "- Opcion√°lis: **chunkol√°s** √°tfed√©ssel (JSONL-hoz)."
    )

st.sidebar.header("Be√°ll√≠t√°sok")
video_labels_str = st.sidebar.text_area(
    "Vide√≥ c√≠mk√©k (soronk√©nt)",
    value="\n".join(DEFAULT_VIDEO_LABELS),
    height=140
)
lesson_labels_str = st.sidebar.text_area(
    "Lecke c√≠mk√©k (soronk√©nt)",
    value="\n".join(DEFAULT_LESSON_LABELS),
    height=120
)
chunk = st.sidebar.checkbox("JSONL chunkol√°sa", value=True)
target_chars = st.sidebar.number_input("Chunk c√©lsz√©less√©g (karakter)", min_value=1000, max_value=20000, value=5500, step=500)
overlap_chars = st.sidebar.number_input("Chunk √°tfed√©s (karakter)", min_value=0, max_value=5000, value=400, step=50)

uploaded = st.file_uploader("T√∂ltsd fel a Notion Markdown ZIP-et", type=["zip"])

if uploaded is not None:
    st.info("ZIP bet√∂ltve. √Åll√≠tsd be a c√≠mk√©ket/param√©tereket, majd ind√≠tsd a konvert√°l√°st.")
    start = st.button("Konvert√°l√°s ind√≠t√°sa", type="primary", use_container_width=True)

    if start:
        vlabels = [x.strip() for x in video_labels_str.splitlines() if x.strip()]
        llabels = [x.strip() for x in lesson_labels_str.splitlines() if x.strip()]

        t0 = time.time()
        (jsonl_bytes, csv_bytes, rep_bytes,
         md_zip_bytes, tables_jsonl_bytes) = convert_zip_to_datasets(
            uploaded.read(), vlabels, llabels, chunk, int(target_chars), int(overlap_chars)
        )

        rid = run_id()

        # Minden egyben (ZIP): JSONL + CSV-k + CLEAN MD-k + t√°bl√°zatok JSONL
        all_buf = io.BytesIO()
        with zipfile.ZipFile(all_buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
            zf.writestr("output.jsonl", jsonl_bytes)
            zf.writestr("output.csv", csv_bytes)         # BOM-os
            zf.writestr("report.csv", rep_bytes)         # BOM-os
            zf.writestr("tables.jsonl", tables_jsonl_bytes)
            # a tiszt√≠tott MD ZIP tartalm√°t al-mappak√©nt bepakoljuk
            with zipfile.ZipFile(io.BytesIO(md_zip_bytes), "r") as mdzf:
                for info in mdzf.infolist():
                    data = mdzf.read(info.filename)
                    zf.writestr(f"clean_md/{info.filename}", data)
        all_buf.seek(0)

        elapsed = int(time.time() - t0)
        st.success(f"K√©sz! ({elapsed} mp)")

        col1, col2, col3 = st.columns(3)
        with col1:
            st.download_button(
                "‚¨áÔ∏è Let√∂lt√©s ‚Äì JSONL (sz√∂veg)",
                data=jsonl_bytes,
                file_name=f"output_{rid}.jsonl",
                mime="application/json"
            )
            st.download_button(
                "‚¨áÔ∏è Let√∂lt√©s ‚Äì CSV",
                data=csv_bytes,
                file_name=f"output_{rid}.csv",
                mime="text/csv; charset=utf-8"
            )
        with col2:
            st.download_button(
                "‚¨áÔ∏è Let√∂lt√©s ‚Äì Riport CSV",
                data=rep_bytes,
                file_name=f"report_{rid}.csv",
                mime="text/csv; charset=utf-8"
            )
            st.download_button(
                "‚¨áÔ∏è Let√∂lt√©s ‚Äì T√°bl√°zatok (JSONL)",
                data=tables_jsonl_bytes,
                file_name=f"tables_{rid}.jsonl",
                mime="application/json"
            )
        with col3:
            st.download_button(
                "‚¨áÔ∏è Let√∂lt√©s ‚Äì Tiszt√≠tott MD-k (ZIP)",
                data=md_zip_bytes,
                file_name=f"clean_md_{rid}.zip",
                mime="application/zip"
            )
            st.download_button(
                "‚¨áÔ∏è Let√∂lt√©s ‚Äì Minden egyben (ZIP)",
                data=all_buf.getvalue(),
                file_name=f"converted_{rid}.zip",
                mime="application/zip"
            )
